{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebok is made wrt. Time series forecasting notebook from Tensorflow https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/time_series.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"/home/guts/Documents/accel_data/2024-10-30_accel_data.csv\"\n",
    "merged_data = pd.read_csv(path)\n",
    "merged_data.index = pd.to_datetime(merged_data['Timestamp'])\n",
    "merged_data.drop('Timestamp', axis=1, inplace=True)\n",
    "merged_data = merged_data[4::5] \n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "_ = merged_data.plot(subplots=True, figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "\n",
    "### Timestamp encoding (Sin/Cos tranformation)\n",
    "\n",
    "In this step, we convert the timestamp into cyclic features using sine and cosine transformations. This is crucial for capturing the cyclical nature of time (e.g., minutes, hours, days) in the data, which helps the model correlate the raw sensor data with time, as the machines often operate in recurring cycles. This makes the LsTM aware of the cyclic nature of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timestamp_s = merged_data.index.map(pd.Timestamp.timestamp)\n",
    "# seconds in a :\n",
    "minute = 60\n",
    "hour = minute*60\n",
    "day = hour*24\n",
    "week = 7*day\n",
    "\n",
    "merged_data['sin_minute'] = np.sin(2*np.pi*timestamp_s/minute)\n",
    "merged_data['cos_minute'] = np.cos(2*np.pi*timestamp_s/minute)\n",
    "\n",
    "merged_data['sin_hour'] = np.sin(2*np.pi*timestamp_s/hour)\n",
    "merged_data['cos_hour'] = np.cos(2*np.pi*timestamp_s/hour)\n",
    "\n",
    "merged_data['sin_day'] = np.sin(2*np.pi*timestamp_s/day)\n",
    "merged_data['cos_day'] = np.cos(2*np.pi*timestamp_s/day)  \n",
    "\n",
    "print(merged_data.head())\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(np.array(merged_data['sin_minute'])[0:60])\n",
    "plt.plot(np.array(merged_data['cos_minute'])[0:60])\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.title('Time of minute signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FFT(Fourier transform)**\n",
    "\n",
    "A Fast Fourier transform (FFT) is applied to the feature to analyze the frequency component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT\n",
    "fft = tf.signal.rfft(merged_data['Linear x'])\n",
    "f_per_dataset = np.arange(0, len(fft))\n",
    "\n",
    "# Number of samples and total duration (in seconds)\n",
    "n_samples_s = len(merged_data['Linear x'])  # Total number of samples\n",
    "seconds_per_day = 24 * 60 * 60  # Seconds in a day\n",
    "hours_per_day = 24  # Hours in a day\n",
    "\n",
    "# Calculate the dataset's duration in days\n",
    "days_per_dataset = n_samples_s / seconds_per_day\n",
    "\n",
    "# Convert frequencies to cycles per day\n",
    "f_per_day = f_per_dataset / days_per_dataset\n",
    "\n",
    "# Plotting\n",
    "plt.step(f_per_day, np.abs(fft))\n",
    "plt.xscale('log')\n",
    "plt.ylim(0, 500)  # Adjust for your amplitude range\n",
    "plt.xlim([0.1, max(plt.xlim())])\n",
    "\n",
    "# Add X-ticks for \"1/day\" and \"1/hour\"\n",
    "plt.xticks([1, 24], labels=['1/hour', '1/day'])\n",
    "\n",
    "# Labels\n",
    "plt.xlabel('Frequency (log scale)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Fourier Transform of Sensor Data')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X-axis (Frequency):**\n",
    "The frequency is shown in logarithmic scale, ranging from lower frequencies (e.g., 1/hour) to higher frequencies (e.g., 1/day). This scale shows how often events or patterns repeat over time.\n",
    "\n",
    "* 1/day: Represents patterns or trends that repeat approximately once per day.\n",
    "* 1/hour: Represents events repeating about once per hour.\n",
    "\n",
    "**Y-axis (Amplitude):**\n",
    "The amplitude represents the strength or magnitude of each frequency component. Higher amplitudes indicate more significant variations at that particular frequency.\n",
    "\n",
    "**Key Observations:**\n",
    "The high amplitude at lower frequencies (around 1/day) indicates that the sensor data has strong periodic patterns at the daily level. This suggests that the sensor readings likely have a cyclical trend related to the daily operation of the machine.\n",
    "As we move to higher frequencies (closer to 1/hour), the amplitude decreases, but there are still some smaller peaks. These could correspond to shorter, but less significant, cyclical patterns within each day or hour.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normalization and Batching\n",
    "The data is normalized and split into windows for the LSTM to process. A sliding window approach is used to feed sequential time segments to the model.\n",
    "\n",
    "**Split the Data**\n",
    "\n",
    "First the data is split into _test, train_ and _val_ datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(merged_data.columns)}\n",
    "\n",
    "n = len(merged_data)\n",
    "train_data = merged_data[0:int(n*0.7)]\n",
    "val_data = merged_data[int(n*0.7):int(n*0.9)]\n",
    "test_data = merged_data[int(n*0.9):]\n",
    "\n",
    "num_features = merged_data.shape[1]\n",
    "num_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_data.mean()\n",
    "train_std = train_data.std()\n",
    "\n",
    "\n",
    "train_data = (train_data - train_mean) / train_std\n",
    "val_data = (val_data - train_mean) / train_std\n",
    "test_data = (test_data - train_mean) / train_std\n",
    "\n",
    "merged_data_std = (merged_data - train_mean) / train_std\n",
    "merged_data_std = merged_data_std.melt(var_name='Column', value_name='Normalized')\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax = sns.violinplot(x='Column', y='Normalized', data=merged_data_std) \n",
    "_ = ax.set_xticklabels(merged_data.keys(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Windowing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idexes and offsets\n",
    "class WindowGenerator:\n",
    "    def __init__(self, input_width, label_width, shift, train_df=train_data,val_df=val_data, test_df=test_data, label_columns=None):\n",
    "        \n",
    "        # Store the raw data\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Work out the label column indices\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in \n",
    "                                          enumerate(label_columns)}\n",
    "\n",
    "        self.column_indices = {name: i for i, name in \n",
    "                               enumerate(train_df.columns)}\n",
    "\n",
    "        # Work out the window parameters\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "  inputs = features[:, self.input_slice, :]\n",
    "  labels = features[:, self.labels_slice, :]\n",
    "  if self.label_columns is not None:\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "  # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  inputs.set_shape([None, self.input_width, None])\n",
    "  labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  return inputs, labels\n",
    "\n",
    "WindowGenerator.split_window = split_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(self, model=None, plot_col='Linear x', max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "                 label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "        if self.label_columns:\n",
    "            label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "        else:\n",
    "            label_col_index = plot_col_index\n",
    "\n",
    "        if label_col_index is None:\n",
    "            continue\n",
    "\n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "            predictions = model(inputs)\n",
    "            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                        marker='X', edgecolors='k', label='Predictions',\n",
    "                        c='#ff7f0e', s=64)\n",
    "            \n",
    "        if n == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [s]')\n",
    "\n",
    "WindowGenerator.plot = plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "\n",
    "def make_dataset(self, data):   \n",
    "    data = np.array(data, dtype=np.float16)\n",
    "    ds  = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=False,\n",
    "        batch_size=32,)\n",
    "    ds = ds.map(self.split_window)\n",
    "    return ds\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        result = next(iter(self.train))\n",
    "        self._example = result\n",
    "    return result\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window = WindowGenerator(\n",
    "    input_width=100, label_width=100, shift=1,\n",
    "    label_columns=['Linear x'])\n",
    "\n",
    "wide_window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cells returns the wide window created but only for the specified feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='min')\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
    "                      validation_data=window.val,\n",
    "                      callbacks=[early_stopping])\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Output model\n",
    "\n",
    "Here, we will create a wide window with multiple feature as outputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wide_window = WindowGenerator(\n",
    "    input_width=10, label_width=10, shift=1)\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(units=num_features)\n",
    "])\n",
    "\n",
    "history = compile_and_fit(lstm_model, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['LSTM'] = lstm_model.evaluate( wide_window.val, return_dict=True)\n",
    "performance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0, return_dict=True)\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(lstm_model, plot_col='Angular z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualWrapper(tf.keras.Model):\n",
    "  def __init__(self, model):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "\n",
    "  def call(self, inputs, *args, **kwargs):\n",
    "    delta = self.model(inputs, *args, **kwargs)\n",
    "\n",
    "    # The prediction for each time step is the input\n",
    "    # from the previous time step plus the delta\n",
    "    # calculated by the model.\n",
    "    return inputs + delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "residual_lstm = ResidualWrapper(\n",
    "    tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dense(\n",
    "        num_features,\n",
    "        # The predicted deltas should start small.\n",
    "        # Therefore, initialize the output layer with zeros.\n",
    "        kernel_initializer=tf.initializers.zeros())\n",
    "]))\n",
    "\n",
    "history = compile_and_fit(residual_lstm, wide_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "val_performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.val, return_dict=True)\n",
    "performance['Residual LSTM'] = residual_lstm.evaluate(wide_window.test, verbose=0, return_dict=True)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_window.plot(residual_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multistep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_STEPS = 50\n",
    "multi_window = WindowGenerator(input_width=50,\n",
    "                               label_width=OUT_STEPS,\n",
    "                               shift=OUT_STEPS)\n",
    "\n",
    "multi_window.plot()\n",
    "multi_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_lstm_model = tf.keras.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "    # Shape => [batch, out_steps*features]\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\n",
    "                          kernel_initializer=tf.initializers.zeros()),\n",
    "    # Shape => [batch, out_steps, features]\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n",
    "])      \n",
    "\n",
    "history = compile_and_fit(multi_lstm_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance = multi_lstm_model.evaluate(multi_window.val)\n",
    "multi_performance = multi_lstm_model.evaluate(multi_window.test, verbose=0)\n",
    "multi_window.plot(multi_lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "  def __init__(self, units, out_steps):\n",
    "    super().__init__()\n",
    "    self.out_steps = out_steps\n",
    "    self.units = units\n",
    "    self.lstm_cell = tf.keras.layers.LSTMCell(units)\n",
    "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_model = FeedBack(units=64, out_steps=OUT_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(self, inputs):\n",
    "  # inputs.shape => (batch, time, features)\n",
    "  # x.shape => (batch, lstm_units)\n",
    "  x, *state = self.lstm_rnn(inputs)\n",
    "\n",
    "  # predictions.shape => (batch, features)\n",
    "  prediction = self.dense(x)\n",
    "  return prediction, state\n",
    "\n",
    "FeedBack.warmup = warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, state = feedback_model.warmup(multi_window.example[0])\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, training=None):\n",
    "  # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "  predictions = []\n",
    "  # Initialize the lstm state\n",
    "  prediction, state = self.warmup(inputs)\n",
    "\n",
    "  # Insert the first prediction\n",
    "  predictions.append(prediction)\n",
    "\n",
    "  # Run the rest of the prediction steps\n",
    "  for n in range(1, self.out_steps):\n",
    "    # Use the last prediction as input.\n",
    "    x = prediction\n",
    "    # Execute one lstm step.\n",
    "    x, state = self.lstm_cell(x, states=state,\n",
    "                              training=training)\n",
    "    # Convert the lstm output to a prediction.\n",
    "    prediction = self.dense(x)\n",
    "    # Add the prediction to the output\n",
    "    predictions.append(prediction)\n",
    "\n",
    "  # predictions.shape => (time, batch, features)\n",
    "  predictions = tf.stack(predictions)\n",
    "  # predictions.shape => (batch, time, features)\n",
    "  predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "  return predictions\n",
    "\n",
    "FeedBack.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output shape (batch, time, features): ', feedback_model(multi_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_val_performance = {}\n",
    "multi_performance = {}\n",
    "\n",
    "history = compile_and_fit(feedback_model, multi_window)\n",
    "\n",
    "IPython.display.clear_output()\n",
    "\n",
    "multi_val_performance['AR LSTM'] = feedback_model.evaluate(multi_window.val)\n",
    "multi_performance['AR LSTM'] = feedback_model.evaluate(multi_window.test, verbose=0)\n",
    "\n",
    "multi_window.plot(feedback_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
